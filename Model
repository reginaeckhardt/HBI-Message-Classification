import preprocessing
import pandas as pd 
import numpy as np
import sklearn as sk
import tensorflow as tf
import matplotlib.pyplot as plt
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.metrics import Precision, Recall
from keras.layers import Conv1D, Embedding, Input, MaxPooling1D
from keras.layers.core import Dense, Dropout, Flatten
from keras.models import Model
from keras.layers.merge import concatenate
from numpy import array
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import one_hot
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
porter = PorterStemmer()

def build_model(vocab_size=10000, max_length=279, kernel_sizes = (3,5,7),
                num_filters = (48,48,48), pool_size=2,dropout_rate=0.5,
                embedding_size=100):
    if len(kernel_sizes) < 1:
        raise ValueError("At least one input stage is required.")
    if len(kernel_sizes) != len(num_filters):
        raise ValueError("Number of filter sizes needs to match number of input stages.")

    inputs = []
    input_stages = []

    for kernel_size, filters in zip(kernel_sizes, num_filters):
        input_ = Input(shape=(max_length,))
        embedding = Embedding(vocab_size, embedding_size)(input_)
        conv = Conv1D(filters=filters, kernel_size=kernel_size, 
                      activation='relu')(embedding)
        conv1 = Conv1D(filters=filters, kernel_size=kernel_size, 
                      activation='relu')(conv)
        drop = Dropout(dropout_rate)(conv1)
        pool = MaxPooling1D(pool_size=pool_size)(drop)
        flat = Flatten()(pool)
    
        inputs.append(input_)
        input_stages.append(flat)

    merged = concatenate(input_stages)

    dense1 = Dense(10, activation='relu')(merged)
    outputs = Dense(4, activation='softmax')(dense1)
    model_cnn = Model(inputs=inputs, outputs=outputs)

    return model_cnn
    
    
    def train_model(max_length=279, vocab_size=10000):
    model = build_model(max_length=max_length, vocab_size=vocab_size)
    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_precision', mode='max', verbose=1, save_best_only=True)
    model.compile(loss='categorical_crossentropy', optimizer='adam',
                  metrics=[Precision(name="precision", thresholds=0.7),
                           Recall(name="recall")])
    history = model.fit([Xtrain, Xtrain, Xtrain], array(ytrain), validation_data=([Xtest, Xtest, Xtest], ytest),
                        epochs=50, verbose=2 ,callbacks=[EarlyStopping("val_precision", patience=10, restore_best_weights=True), checkpoint])
    training_loss = history.history['loss']
    test_loss = history.history['val_loss']

    # Create count of the number of epochs
    epoch_count = range(1, len(training_loss) + 1)

    # Visualize loss history
    plt.plot(epoch_count, training_loss, 'r--')
    plt.plot(epoch_count, test_loss, 'b-')
    plt.legend(['Training Loss', 'Test Loss'])
    plt.title('Model Loss CNN')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.show();
    return model, history
